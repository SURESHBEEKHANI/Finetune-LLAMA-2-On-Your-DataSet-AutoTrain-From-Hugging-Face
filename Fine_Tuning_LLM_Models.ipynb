{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyNQOurIA0jCF6r25YVQSp0y",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/SURESHBEEKHANI/Finetune-LLAMA-2-On-Your-DataSet-AutoTrain-From-Hugging-Face/blob/main/Fine_Tuning_LLM_Models.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Step 1: Install All the Required Packages"
      ],
      "metadata": {
        "id": "XYAhNHuPcsmt"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 1: Import Libraries"
      ],
      "metadata": {
        "id": "iOQ4NVuQU7Cy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import necessary libraries for model training and fine-tuning.\n",
        "import torch  # For PyTorch functionality.\n",
        "import pandas as pd  # For handling data in DataFrame format.\n",
        "from datasets import Dataset  # For working with Hugging Face datasets.\n",
        "from transformers import TrainingArguments, TextStreamer  # For training arguments and text streaming.\n",
        "from unsloth.chat_templates import get_chat_template  # For applying chat templates.\n",
        "from unsloth import FastLanguageModel, is_bfloat16_supported  # For loading and configuring the model.\n"
      ],
      "metadata": {
        "id": "JcsbxmCvQvwZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#  Part 2: Define Model and Tokenizer"
      ],
      "metadata": {
        "id": "OSs3TWTnVgqX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the maximum sequence length for the model input.\n",
        "max_seq_length = 2048  # Adjust based on your model's capabilities.\n",
        "\n",
        "# Load the pre-trained model with 4-bit quantization for efficiency.\n",
        "model, tokenizer = FastLanguageModel.from_pretrained(\n",
        "    model_name=\"unsloth/Meta-Llama-3.1-8B-bnb-4bit\",  # Model name with 4-bit precision.\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length for the model.\n",
        "    load_in_4bit=True,  # Load the model in 4-bit precision to save memory.\n",
        "    dtype=None,  # Use default data type.\n",
        ")\n",
        "\n",
        "# Prepare the model for parameter-efficient fine-tuning with LoRA adapters.\n",
        "model = FastLanguageModel.get_peft_model(\n",
        "    model,\n",
        "    r=16,  # Rank of LoRA adapters.\n",
        "    lora_alpha=16,  # Scaling factor for LoRA adapters.\n",
        "    lora_dropout=0,  # Dropout rate for LoRA adapters.\n",
        "    target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"up_proj\", \"down_proj\", \"o_proj\", \"gate_proj\"],\n",
        "    # Modules in the model where LoRA adapters will be applied.\n",
        "    use_rslora=True,  # Use Rank-Stabilized LoRA.\n",
        "    use_gradient_checkpointing=\"unsloth\"  # Use gradient checkpointing.\n",
        ")\n"
      ],
      "metadata": {
        "id": "tOdmqp93Vlyv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 3: Load Local Data"
      ],
      "metadata": {
        "id": "SVBDk702Vxqu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load local CSV file.\n",
        "local_file_path = \"path/to/your/local_file.csv\"  # Update with your file path.\n",
        "df = pd.read_csv(local_file_path)  # Load CSV file into a DataFrame.\n",
        "\n",
        "# Convert DataFrame to Hugging Face dataset.\n",
        "dataset = Dataset.from_pandas(df)\n"
      ],
      "metadata": {
        "id": "sjzZM_MsVuXl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 4: Apply Chat Template"
      ],
      "metadata": {
        "id": "GjKNd2fKV-Lk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Apply a chat template to the dataset to structure the conversations.\n",
        "tokenizer = get_chat_template(\n",
        "    tokenizer,\n",
        "    mapping={\"role\": \"from\", \"content\": \"value\", \"user\": \"human\", \"assistant\": \"gpt\"},\n",
        "    chat_template=\"chatml\",  # Use the ChatML template.\n",
        ")\n",
        "\n",
        "# Function to apply the chat template to dataset examples.\n",
        "def apply_template(examples):\n",
        "    messages = examples[\"conversations\"]  # Extract messages from the dataset.\n",
        "    text = [tokenizer.apply_chat_template(message, tokenize=False, add_generation_prompt=False) for message in messages]\n",
        "    return {\"text\": text}  # Return formatted text.\n",
        "\n",
        "# Apply the template function to the dataset.\n",
        "dataset = dataset.map(apply_template, batched=True)\n"
      ],
      "metadata": {
        "id": "Zb76ra1fV5M9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 5: Set Up Training\n"
      ],
      "metadata": {
        "id": "1YxdJ5hpWKFs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Define the training parameters for the model.\n",
        "from transformers import TrainingArguments\n",
        "from trl import SFTTrainer\n",
        "from unsloth import is_bfloat16_supported\n",
        "\n",
        "# Initialize the SFTTrainer with training parameters.\n",
        "trainer = SFTTrainer(\n",
        "    model=model,  # Model to be trained.\n",
        "    tokenizer=tokenizer,  # Tokenizer used for encoding inputs.\n",
        "    train_dataset=dataset,  # Dataset used for training.\n",
        "    dataset_text_field=\"text\",  # Field in the dataset that contains the text data.\n",
        "    max_seq_length=max_seq_length,  # Maximum sequence length for training.\n",
        "    dataset_num_proc=2,  # Number of processes for data processing.\n",
        "    packing=True,  # Enable packing to handle variable-length sequences.\n",
        "    args=TrainingArguments(\n",
        "        learning_rate=3e-4,  # Learning rate for the optimizer.\n",
        "        lr_scheduler_type=\"linear\",  # Learning rate scheduler type.\n",
        "        per_device_train_batch_size=8,  # Batch size per device.\n",
        "        gradient_accumulation_steps=2,  # Number of steps to accumulate gradients.\n",
        "        num_train_epochs=1,  # Number of epochs for training.\n",
        "        fp16=not is_bfloat16_supported(),  # Use FP16 if BF16 is not supported.\n",
        "        bf16=is_bfloat16_supported(),  # Use BF16 if supported.\n",
        "        logging_steps=1,  # Log every step.\n",
        "        optim=\"adamw_8bit\",  # Optimizer to use.\n",
        "        weight_decay=0.01,  # Weight decay for regularization.\n",
        "        warmup_steps=10,  # Number of steps for learning rate warmup.\n",
        "        output_dir=\"output\",  # Directory to save model checkpoints.\n",
        "        seed=0,  # Random seed for reproducibility.\n",
        "    ),\n",
        ")\n",
        "\n",
        "# Start the training process.\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "hcNG8Ee9WZA0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Part 6: Save and Push Model"
      ],
      "metadata": {
        "id": "bQx006HJWQSM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Prepare the model for inference.\n",
        "from transformers import TextStreamer\n",
        "\n",
        "model = FastLanguageModel.for_inference(model)  # Convert model for faster inference.\n",
        "\n",
        "# Define a test prompt and prepare it for the model.\n",
        "messages = [\n",
        "    {\"from\": \"human\", \"value\": \"Is 9.11 larger than 9.9?\"},  # Test prompt.\n",
        "]\n",
        "inputs = tokenizer.apply_chat_template(\n",
        "    messages,\n",
        "    tokenize=True,\n",
        "    add_generation_prompt=True,\n",
        "    return_tensors=\"pt\",\n",
        ").to(\"cuda\")  # Prepare inputs and move them to GPU.\n",
        "\n",
        "# Generate a response from the model.\n",
        "text_streamer = TextStreamer(tokenizer)  # Stream the output text.\n",
        "_ = model.generate(input_ids=inputs, streamer=text_streamer, max_new_tokens=128, use_cache=True)\n",
        "\n",
        "# Save the trained model with adapters.\n",
        "model.save_pretrained_merged(\"model\", tokenizer, save_method=\"merged_16bit\")  # Save in 16-bit precision.\n",
        "model.push_to_hub_merged(\"mlabonne/FineLlama-3.1-8B\", tokenizer, save_method=\"merged_16bit\")  # Push to Hugging Face Hub.\n",
        "\n",
        "# Convert and save the model in different quantization formats.\n",
        "quant_methods = [\"q2_k\", \"q3_k_m\", \"q4_k_m\", \"q5_k_m\", \"q6_k\", \"q8_0\"]  # List of quantization methods.\n",
        "for quant in quant_methods:\n",
        "    model.push_to_hub_gguf(\"mlabonne/FineLlama-3.1-8B-GGUF\", tokenizer, quant)  # Push quantized models to Hugging Face Hub.\n",
        "\n",
        "# Print a message indicating successful fine-tuning and model uploading.\n",
        "print(\"Congratulations, we fine-tuned a model from scratch and uploaded quants you can now use in your favorite inference engine.\")\n"
      ],
      "metadata": {
        "id": "ejHePGq9WLk8"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}